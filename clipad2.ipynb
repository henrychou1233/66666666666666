{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895aa803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# mvtec2_clip_pipeline.ipynb\n",
    "# 整合版 Pipeline：gen_txt + train + inference + 統計 AUROC\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# ================== 全域設定 ==================\n",
    "MVTEC2_CLASSES = [\n",
    "    \"fabric\"\n",
    "]\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\\dataset\\mvtec2\")\n",
    "CHECKPOINT_ROOT = Path(\"clipcheckpoints\")\n",
    "OUT_BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\DDAD-main\")\n",
    "RESULTS_CSV = OUT_BASE_DIR / \"mvtec2_clip_results.csv\"\n",
    "\n",
    "IMGSIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 跑三組 (3,3) (4,4) (5,5)\n",
    "CONFIGS = [(3,3), (4,4), (5,5)]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Part 1. gen_txt (生成 txt + clip2/clip4)\n",
    "# ======================================================\n",
    "NORMAL_PROMPTS = [\"a flawless {}\", \"a good {}\", \"a perfect {}\"]\n",
    "ANOMALY_PROMPTS = [\n",
    "    \"a defective {}\", \"a broken {}\", \"a damaged {}\", \"a cracked {}\",\n",
    "    \"a faulty {}\", \"a scratched {}\", \"a {} with visible defects\",\n",
    "    \"a malfunctioning {}\", \"a {} with flaws\", \"a deteriorated {}\"\n",
    "]\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\"}\n",
    "TXT_SUFFIX = \".txt\"\n",
    "IMG_COPY_FORMAT = \"{:03d}.png\"\n",
    "TXT_COPY_FORMAT = \"{:03d}.txt\"\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, folder: Path, clip2_aug_num: int, clip4_aug_num: int):\n",
    "        self.folder = folder\n",
    "        self.images = sorted([p for p in self.folder.iterdir() if p.suffix.lower() in IMG_EXTS])\n",
    "        self.class_name = self.folder.parent.parent.name\n",
    "        self.clip2_aug_num = clip2_aug_num\n",
    "        self.clip4_aug_num = clip4_aug_num\n",
    "\n",
    "    def reset_dir(self, out_dir: Path):\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for file in out_dir.iterdir():\n",
    "            if file.is_file():\n",
    "                file.unlink()\n",
    "            elif file.is_dir():\n",
    "                shutil.rmtree(file)\n",
    "\n",
    "    def _save_resized(self, src_img: Image.Image, dst_img: Path):\n",
    "        src_img.resize((IMGSIZE, IMGSIZE), Image.BICUBIC).save(dst_img)\n",
    "\n",
    "    def generate_good_prompts(self):\n",
    "        for img_path in tqdm(self.images, desc=f\"[{self.class_name}] GOOD\", unit=\"img\"):\n",
    "            txt_path = img_path.with_suffix(TXT_SUFFIX)\n",
    "            prompt = random.choice(NORMAL_PROMPTS).format(self.class_name)\n",
    "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(prompt)\n",
    "\n",
    "    def augment_good_prompts(self, out_dir: Path):\n",
    "        self.reset_dir(out_dir)\n",
    "        counter = 0\n",
    "        for img_path in tqdm(self.images, desc=f\"[{self.class_name}] CLIP2\", unit=\"img\"):\n",
    "            for k in range(self.clip2_aug_num):\n",
    "                dst_img = out_dir / IMG_COPY_FORMAT.format(counter)\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                self._save_resized(img, dst_img)\n",
    "                txt_path = out_dir / TXT_COPY_FORMAT.format(counter)\n",
    "                prompt = random.choice(NORMAL_PROMPTS).format(self.class_name)\n",
    "                with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(prompt)\n",
    "                counter += 1\n",
    "\n",
    "    def augment_good_clip4(self, out_dir: Path):\n",
    "        self.reset_dir(out_dir)\n",
    "        counter = 0\n",
    "        for img_path in tqdm(self.images, desc=f\"[{self.class_name}] CLIP4\", unit=\"img\"):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            for k in range(self.clip4_aug_num):\n",
    "                angle = random.choice([0, 15, -15, 30, -30])\n",
    "                scale = random.uniform(0.8, 1.2)\n",
    "                w, h = img.size\n",
    "                new_w, new_h = int(w * scale), int(h * scale)\n",
    "                aug_img = img.resize((new_w, new_h), Image.BICUBIC).rotate(angle, expand=True)\n",
    "                if random.random() < 0.5:\n",
    "                    aug_img = ImageOps.mirror(aug_img)\n",
    "                if random.random() < 0.3:\n",
    "                    aug_img = ImageOps.flip(aug_img)\n",
    "                dst_img = out_dir / IMG_COPY_FORMAT.format(counter)\n",
    "                self._save_resized(aug_img, dst_img)\n",
    "                txt_path = out_dir / TXT_COPY_FORMAT.format(counter)\n",
    "                prompt = random.choice(NORMAL_PROMPTS).format(self.class_name)\n",
    "                with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(prompt)\n",
    "                counter += 1\n",
    "\n",
    "def run_gen_txt(cls, clip2_num, clip4_num):\n",
    "    src_dir = BASE_DIR / cls / \"train\" / \"good\"\n",
    "    gen = TextGenerator(src_dir, clip2_num, clip4_num)\n",
    "    gen.generate_good_prompts()\n",
    "    gen.augment_good_prompts(src_dir.parent / \"clip2\")\n",
    "    gen.augment_good_clip4(src_dir.parent / \"clip4\")\n",
    "\n",
    "# ======================================================\n",
    "# Part 2. train_clip (訓練)\n",
    "# ======================================================\n",
    "class ImageTextPairDataset(Dataset):\n",
    "    def __init__(self, base_dir: Path):\n",
    "        self.samples = []\n",
    "        self.class_name = base_dir.name\n",
    "        for sub in [\"good\", \"clip2\", \"clip4\"]:\n",
    "            p = base_dir / f\"train/{sub}\"\n",
    "            if not p.exists():\n",
    "                continue\n",
    "            for img_path in sorted(p.glob(\"*.png\")):\n",
    "                txt_path = img_path.with_suffix(\".txt\")\n",
    "                if txt_path.exists():\n",
    "                    self.samples.append((img_path, txt_path))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, txt_path = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "        text = Path(txt_path).read_text().strip()\n",
    "        return {\"image\": img, \"text\": text, \"path\": str(img_path)}\n",
    "\n",
    "def collate_fn(batch, processor):\n",
    "    images = [b[\"image\"] for b in batch]\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    inputs[\"paths\"] = [b[\"path\"] for b in batch]\n",
    "    return inputs\n",
    "\n",
    "def clip_loss(image_embeds, text_embeds, logit_scale):\n",
    "    img = image_embeds / (image_embeds.norm(dim=-1, keepdim=True)+1e-12)\n",
    "    txt = text_embeds / (text_embeds.norm(dim=-1, keepdim=True)+1e-12)\n",
    "    logits = torch.matmul(img, txt.t()) * logit_scale\n",
    "    labels = torch.arange(img.size(0), device=img.device)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits, labels)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits.t(), labels)\n",
    "    return (loss_i + loss_t) / 2.0\n",
    "\n",
    "def run_train_clip(cls):\n",
    "    base_dir = BASE_DIR / cls\n",
    "    dataset = ImageTextPairDataset(base_dir)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True,\n",
    "                            collate_fn=lambda b: collate_fn(b, processor))\n",
    "\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-7)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=2)\n",
    "\n",
    "    ckpt_root = CHECKPOINT_ROOT / cls\n",
    "    ckpt_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(2):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"[{cls}] Epoch {epoch+1}\", unit=\"batch\"):\n",
    "            inputs = {k:v.to(DEVICE) for k,v in batch.items() if k!=\"paths\"}\n",
    "            outputs = model(**inputs)\n",
    "            loss = clip_loss(outputs.image_embeds, outputs.text_embeds, model.logit_scale.exp())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        print(f\"[{cls}] Epoch {epoch+1} - AvgLoss {total_loss/len(dataloader):.6f}\")\n",
    "        model.save_pretrained(ckpt_root / f\"epoch_{epoch+1}\")\n",
    "        processor.save_pretrained(ckpt_root / f\"epoch_{epoch+1}\")\n",
    "\n",
    "# ======================================================\n",
    "# Part 3. inference (計算 AUROC)\n",
    "# ======================================================\n",
    "def anomaly_score(model, processor, image, text_prompt):\n",
    "    inputs = processor(images=image, text=[text_prompt], return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    img = outputs.image_embeds\n",
    "    txt = outputs.text_embeds\n",
    "    sim = torch.matmul(img/img.norm(dim=-1,keepdim=True),\n",
    "                       (txt/txt.norm(dim=-1,keepdim=True)).t()).item()\n",
    "    return 1.0 - sim\n",
    "\n",
    "def run_inference(cls, prompt=\"a flawless {}\"):\n",
    "    model_dir = CHECKPOINT_ROOT / cls / \"epoch_2\"\n",
    "    processor = CLIPProcessor.from_pretrained(str(model_dir))\n",
    "    model = CLIPModel.from_pretrained(str(model_dir)).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    test_root = BASE_DIR / cls / \"test\"\n",
    "    y_true, y_score = [], []\n",
    "    for sub in [\"good\"] + [d.name for d in test_root.iterdir() if d.is_dir() and d.name!=\"good\"]:\n",
    "        label = 0 if sub==\"good\" else 1\n",
    "        for img_path in (test_root/sub).glob(\"*.png\"):\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "            score = anomaly_score(model, processor, img, prompt.format(cls))\n",
    "            y_true.append(label); y_score.append(score)\n",
    "\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "# ======================================================\n",
    "# Part 4. Pipeline 主程式\n",
    "# ======================================================\n",
    "results = []\n",
    "for c2, c4 in CONFIGS:\n",
    "    print(f\"\\n===== Running config (CLIP2={c2}, CLIP4={c4}) =====\")\n",
    "    for cls in MVTEC2_CLASSES:\n",
    "        print(f\"\\n>>> Class: {cls}\")\n",
    "        run_gen_txt(cls, c2, c4)\n",
    "        run_train_clip(cls)\n",
    "        auroc = run_inference(cls)\n",
    "        print(f\"[RESULT] {cls} clip2={c2} clip4={c4} AUROC={auroc:.4f}\")\n",
    "        results.append({\"class\": cls, \"clip2\": c2, \"clip4\": c4, \"auroc\": auroc})\n",
    "\n",
    "# 存檔\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RESULTS_CSV, index=False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653df0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Class: one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[one] 推理中: 100%|██████████| 70/70 [03:00<00:00,  2.57s/img]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] one AUROC=0.7123\n",
      "Results saved to C:\\Users\\anywhere4090\\Desktop\\DDAD-main\\mvtec2_winclip_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# mvtec2_winclip_pipeline.py\n",
    "# WinCLIP Pipeline：CPE prompt + window-based feature extraction + AUROC (含進度條)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import open_clip   # pip install open_clip_torch\n",
    "\n",
    "# ================== 全域設定 ==================\n",
    "MVTEC2_CLASSES = [\"one\"]\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\\dataset\\btad\")\n",
    "OUT_BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\DDAD-main\")\n",
    "RESULTS_CSV = OUT_BASE_DIR / \"mvtec2_winclip_results.csv\"\n",
    "\n",
    "IMGSIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Window/patch 設定\n",
    "WINDOW_SIZES = [32, 48, 64]\n",
    "STRIDE = 16\n",
    "\n",
    "# ================== CPE Prompt ==================\n",
    "STATE_WORDS_NORMAL = [\"flawless\", \"intact\", \"perfect\", \"clean\", \"good\"]\n",
    "STATE_WORDS_ANOM = [\"broken\", \"cracked\", \"damaged\", \"scratched\", \"defective\", \"faulty\"]\n",
    "TEMPLATES = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of a {} for visual inspection\"\n",
    "]\n",
    "\n",
    "def build_cpe_prompts(cls_name):\n",
    "    normal_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_NORMAL for t in TEMPLATES]\n",
    "    anomaly_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_ANOM for t in TEMPLATES]\n",
    "    return normal_prompts, anomaly_prompts\n",
    "\n",
    "# ================== WinCLIP 特徵抽取 ==================\n",
    "def extract_winclip_features(model, preprocess, image, normal_prompts, anomaly_prompts,\n",
    "                             window_size=32, stride=16):\n",
    "    W, H = image.size\n",
    "    scores = np.zeros((H, W))\n",
    "    counts = np.zeros((H, W))\n",
    "\n",
    "    # encode text prompts\n",
    "    with torch.no_grad():\n",
    "        txt_norm = model.encode_text(open_clip.tokenize(normal_prompts).to(DEVICE))\n",
    "        txt_anom = model.encode_text(open_clip.tokenize(anomaly_prompts).to(DEVICE))\n",
    "        txt_norm = txt_norm / txt_norm.norm(dim=-1, keepdim=True)\n",
    "        txt_anom = txt_anom / txt_anom.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # sliding window\n",
    "    for y in range(0, H - window_size + 1, stride):\n",
    "        for x in range(0, W - window_size + 1, stride):\n",
    "            crop = image.crop((x, y, x + window_size, y + window_size))\n",
    "            crop_tensor = preprocess(crop).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img_emb = model.encode_image(crop_tensor)\n",
    "                img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            sim_norm = (img_emb @ txt_norm.T).max().item()\n",
    "            sim_anom = (img_emb @ txt_anom.T).max().item()\n",
    "            anomaly_score = sim_anom - sim_norm  # 越大越異常\n",
    "\n",
    "            scores[y:y + window_size, x:x + window_size] += anomaly_score\n",
    "            counts[y:y + window_size, x:x + window_size] += 1\n",
    "\n",
    "    return scores / (counts + 1e-6)\n",
    "\n",
    "# ================== 推理 (WinCLIP Zero-shot) ==================\n",
    "def run_inference_winclip(cls):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        \"ViT-B-16\", pretrained=\"laion400m_e32\"\n",
    "    )\n",
    "    model = model.to(DEVICE).eval()\n",
    "\n",
    "    test_root = BASE_DIR / cls / \"test\"\n",
    "    y_true, y_score = [], []\n",
    "    normal_prompts, anomaly_prompts = build_cpe_prompts(cls)\n",
    "\n",
    "    # 計算總影像數量，用於 tqdm\n",
    "    all_imgs = []\n",
    "    for sub in [\"good\"] + [d.name for d in test_root.iterdir() if d.is_dir() and d.name != \"good\"]:\n",
    "        for img_path in (test_root / sub).glob(\"*.png\"):\n",
    "            all_imgs.append((img_path, sub))\n",
    "    \n",
    "    for img_path, sub in tqdm(all_imgs, desc=f\"[{cls}] 推理中\", unit=\"img\"):\n",
    "        label = 0 if sub == \"good\" else 1\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "\n",
    "        # 多尺度融合\n",
    "        score_maps = []\n",
    "        for ws in WINDOW_SIZES:\n",
    "            score_map = extract_winclip_features(\n",
    "                model, preprocess, img, normal_prompts, anomaly_prompts,\n",
    "                window_size=ws, stride=STRIDE\n",
    "            )\n",
    "            score_maps.append(score_map)\n",
    "\n",
    "        final_map = np.mean(score_maps, axis=0)\n",
    "        score = final_map.max()  # image-level score\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_score.append(score)\n",
    "\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "# ================== Pipeline 主程式 ==================\n",
    "results = []\n",
    "for cls in MVTEC2_CLASSES:\n",
    "    print(f\"\\n>>> Class: {cls}\")\n",
    "    auroc = run_inference_winclip(cls)\n",
    "    print(f\"[RESULT] {cls} AUROC={auroc:.4f}\")\n",
    "    results.append({\"class\": cls, \"auroc\": auroc})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"Results saved to {RESULTS_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39b1edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Class: one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[one] 推理中: 100%|██████████| 70/70 [02:49<00:00,  2.42s/img]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] one AUROC=0.8873\n",
      "Results saved to C:\\Users\\anywhere4090\\Desktop\\DDAD-main\\mvtec2_winclip_plus_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# mvtec2_winclip_plus_pipeline.py\n",
    "# WinCLIP+ Pipeline：CPE prompt + Few-shot Normal Support + window-based feature extraction + AUROC (含進度條)\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import open_clip   # pip install open_clip_torch\n",
    "\n",
    "# ================== 全域設定 ==================\n",
    "MVTEC2_CLASSES = [\"one\"]\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\\dataset\\btad\")\n",
    "OUT_BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\DDAD-main\")\n",
    "RESULTS_CSV = OUT_BASE_DIR / \"mvtec2_winclip_plus_results.csv\"\n",
    "\n",
    "IMGSIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Window/patch 設定\n",
    "WINDOW_SIZES = [32, 48, 64]\n",
    "STRIDE = 16\n",
    "\n",
    "# ================== CPE Prompt ==================\n",
    "STATE_WORDS_NORMAL = [\"flawless\", \"intact\", \"perfect\", \"clean\", \"good\"]\n",
    "STATE_WORDS_ANOM = [\"broken\", \"cracked\", \"damaged\", \"scratched\", \"defective\", \"faulty\"]\n",
    "TEMPLATES = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of a {} for visual inspection\"\n",
    "]\n",
    "\n",
    "def build_cpe_prompts(cls_name):\n",
    "    normal_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_NORMAL for t in TEMPLATES]\n",
    "    anomaly_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_ANOM for t in TEMPLATES]\n",
    "    return normal_prompts, anomaly_prompts\n",
    "\n",
    "# ================== 建立 Support Embeddings ==================\n",
    "def build_support_embeddings(model, preprocess, cls, max_support=5):\n",
    "    \"\"\"從 train/good 抽少量正常影像，生成 support embedding\"\"\"\n",
    "    support_dir = BASE_DIR / cls / \"train\" / \"good\"\n",
    "    support_imgs = sorted(list(support_dir.glob(\"*.png\")))[:max_support]\n",
    "    support_embs = []\n",
    "\n",
    "    for img_path in support_imgs:\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "        tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            emb = model.encode_image(tensor)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        support_embs.append(emb)\n",
    "\n",
    "    if len(support_embs) > 0:\n",
    "        return torch.cat(support_embs, dim=0)  # shape = [N, D]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# ================== WinCLIP+ 特徵抽取 ==================\n",
    "def extract_winclip_plus_features(model, preprocess, image, normal_prompts, anomaly_prompts,\n",
    "                                  emb_norm_support=None, window_size=32, stride=16):\n",
    "    W, H = image.size\n",
    "    scores = np.zeros((H, W))\n",
    "    counts = np.zeros((H, W))\n",
    "\n",
    "    # encode text prompts\n",
    "    with torch.no_grad():\n",
    "        txt_norm = model.encode_text(open_clip.tokenize(normal_prompts).to(DEVICE))\n",
    "        txt_anom = model.encode_text(open_clip.tokenize(anomaly_prompts).to(DEVICE))\n",
    "        txt_norm = txt_norm / txt_norm.norm(dim=-1, keepdim=True)\n",
    "        txt_anom = txt_anom / txt_anom.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # sliding window\n",
    "    for y in range(0, H - window_size + 1, stride):\n",
    "        for x in range(0, W - window_size + 1, stride):\n",
    "            crop = image.crop((x, y, x + window_size, y + window_size))\n",
    "            crop_tensor = preprocess(crop).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img_emb = model.encode_image(crop_tensor)\n",
    "                img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # normal 相似度 (文字 + 支援影像)\n",
    "            sim_norm_text = (img_emb @ txt_norm.T).max().item()\n",
    "            sim_norm_support = (img_emb @ emb_norm_support.T).max().item() if emb_norm_support is not None else -1e9\n",
    "            sim_norm = max(sim_norm_text, sim_norm_support)\n",
    "\n",
    "            # anomaly 相似度\n",
    "            sim_anom = (img_emb @ txt_anom.T).max().item()\n",
    "\n",
    "            anomaly_score = sim_anom - sim_norm  # 越大越異常\n",
    "\n",
    "            scores[y:y + window_size, x:x + window_size] += anomaly_score\n",
    "            counts[y:y + window_size, x:x + window_size] += 1\n",
    "\n",
    "    return scores / (counts + 1e-6)\n",
    "\n",
    "# ================== 推理 (WinCLIP+) ==================\n",
    "def run_inference_winclip_plus(cls, max_support=10):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        \"ViT-B-16\", pretrained=\"laion400m_e32\"\n",
    "    )\n",
    "    model = model.to(DEVICE).eval()\n",
    "\n",
    "    # 建立 support embeddings\n",
    "    emb_norm_support = build_support_embeddings(model, preprocess, cls, max_support=max_support)\n",
    "\n",
    "    test_root = BASE_DIR / cls / \"test\"\n",
    "    y_true, y_score = [], []\n",
    "    normal_prompts, anomaly_prompts = build_cpe_prompts(cls)\n",
    "\n",
    "    # 收集所有測試影像\n",
    "    all_imgs = []\n",
    "    for sub in [\"good\"] + [d.name for d in test_root.iterdir() if d.is_dir() and d.name != \"good\"]:\n",
    "        for img_path in (test_root / sub).glob(\"*.png\"):\n",
    "            all_imgs.append((img_path, sub))\n",
    "    \n",
    "    for img_path, sub in tqdm(all_imgs, desc=f\"[{cls}] 推理中\", unit=\"img\"):\n",
    "        label = 0 if sub == \"good\" else 1\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "\n",
    "        # 多尺度融合\n",
    "        score_maps = []\n",
    "        for ws in WINDOW_SIZES:\n",
    "            score_map = extract_winclip_plus_features(\n",
    "                model, preprocess, img, normal_prompts, anomaly_prompts,\n",
    "                emb_norm_support=emb_norm_support,\n",
    "                window_size=ws, stride=STRIDE\n",
    "            )\n",
    "            score_maps.append(score_map)\n",
    "\n",
    "        final_map = np.mean(score_maps, axis=0)\n",
    "        score = final_map.max()  # image-level score\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_score.append(score)\n",
    "\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "# ================== Pipeline 主程式 ==================\n",
    "results = []\n",
    "for cls in MVTEC2_CLASSES:\n",
    "    print(f\"\\n>>> Class: {cls}\")\n",
    "    auroc = run_inference_winclip_plus(cls, max_support=5)\n",
    "    print(f\"[RESULT] {cls} AUROC={auroc:.4f}\")\n",
    "    results.append({\"class\": cls, \"auroc\": auroc})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"Results saved to {RESULTS_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653fd651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Class: one (Backbone=ViT-B-16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[one] 推理中: 100%|██████████| 70/70 [02:56<00:00,  2.52s/img]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] one AUROC=0.9349\n",
      "Results saved to C:\\Users\\anywhere4090\\Desktop\\DDAD-main\\mvtec2_winclip_plus_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# mvtec2_winclip_plus_pipeline.py\n",
    "# WinCLIP+ Pipeline：CPE prompt + Few-shot Normal Support + Top-k pooling + backbone selection\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import open_clip   # pip install open_clip_torch\n",
    "\n",
    "# ================== 全域設定 ==================\n",
    "MVTEC2_CLASSES = [\"one\"]\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\\dataset\\btad\")\n",
    "OUT_BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\DDAD-main\")\n",
    "RESULTS_CSV = OUT_BASE_DIR / \"mvtec2_winclip_plus_results.csv\"\n",
    "\n",
    "IMGSIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Backbone 可選: \"ViT-B-16\" / \"ViT-L-14\"\n",
    "BACKBONE = \"ViT-B-16\"\n",
    "PRETRAINED = \"laion400m_e32\" if BACKBONE == \"ViT-B-16\" else \"laion2b_s32b_b82k\"\n",
    "\n",
    "# Window/patch 設定\n",
    "WINDOW_SIZES = [32, 48, 64]\n",
    "STRIDE = 16\n",
    "\n",
    "# Top-k pooling 設定\n",
    "TOPK_RATIO = 0.05  # 取前 5%\n",
    "\n",
    "# ================== CPE Prompt ==================\n",
    "STATE_WORDS_NORMAL = [\"flawless\", \"intact\", \"perfect\", \"clean\", \"good\"]\n",
    "STATE_WORDS_ANOM = [\"broken\", \"cracked\", \"damaged\", \"scratched\", \"defective\", \"faulty\"]\n",
    "TEMPLATES = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of a {} for visual inspection\"\n",
    "]\n",
    "\n",
    "def build_cpe_prompts(cls_name):\n",
    "    normal_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_NORMAL for t in TEMPLATES]\n",
    "    anomaly_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_ANOM for t in TEMPLATES]\n",
    "    return normal_prompts, anomaly_prompts\n",
    "\n",
    "# ================== 建立 Support Embeddings ==================\n",
    "def build_support_embeddings(model, preprocess, cls, max_support=9999):\n",
    "    \"\"\"使用 train/good 生成 support embedding (可用全部正常影像)\"\"\"\n",
    "    support_dir = BASE_DIR / cls / \"train\" / \"good\"\n",
    "    support_imgs = sorted(list(support_dir.glob(\"*.png\")))[:max_support]\n",
    "    support_embs = []\n",
    "\n",
    "    for img_path in support_imgs:\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "        tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            emb = model.encode_image(tensor)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        support_embs.append(emb)\n",
    "\n",
    "    if len(support_embs) > 0:\n",
    "        return torch.cat(support_embs, dim=0)  # shape = [N, D]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# ================== WinCLIP+ 特徵抽取 ==================\n",
    "def extract_winclip_plus_features(model, preprocess, image, normal_prompts, anomaly_prompts,\n",
    "                                  emb_norm_support=None, window_size=32, stride=16):\n",
    "    W, H = image.size\n",
    "    scores = np.zeros((H, W))\n",
    "    counts = np.zeros((H, W))\n",
    "\n",
    "    # encode text prompts\n",
    "    with torch.no_grad():\n",
    "        txt_norm = model.encode_text(open_clip.tokenize(normal_prompts).to(DEVICE))\n",
    "        txt_anom = model.encode_text(open_clip.tokenize(anomaly_prompts).to(DEVICE))\n",
    "        txt_norm = txt_norm / txt_norm.norm(dim=-1, keepdim=True)\n",
    "        txt_anom = txt_anom / txt_anom.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # sliding window\n",
    "    for y in range(0, H - window_size + 1, stride):\n",
    "        for x in range(0, W - window_size + 1, stride):\n",
    "            crop = image.crop((x, y, x + window_size, y + window_size))\n",
    "            crop_tensor = preprocess(crop).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img_emb = model.encode_image(crop_tensor)\n",
    "                img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # normal 相似度 (文字 + 支援影像)\n",
    "            sim_norm_text = (img_emb @ txt_norm.T).max().item()\n",
    "            sim_norm_support = (img_emb @ emb_norm_support.T).max().item() if emb_norm_support is not None else -1e9\n",
    "            sim_norm = max(sim_norm_text, sim_norm_support)\n",
    "\n",
    "            # anomaly 相似度\n",
    "            sim_anom = (img_emb @ txt_anom.T).max().item()\n",
    "\n",
    "            anomaly_score = sim_anom - sim_norm  # 越大越異常\n",
    "\n",
    "            scores[y:y + window_size, x:x + window_size] += anomaly_score\n",
    "            counts[y:y + window_size, x:x + window_size] += 1\n",
    "\n",
    "    return scores / (counts + 1e-6)\n",
    "\n",
    "# ================== 推理 (WinCLIP+) ==================\n",
    "def run_inference_winclip_plus(cls, max_support=9999):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        BACKBONE, pretrained=PRETRAINED\n",
    "    )\n",
    "    model = model.to(DEVICE).eval()\n",
    "\n",
    "    # 建立 support embeddings\n",
    "    emb_norm_support = build_support_embeddings(model, preprocess, cls, max_support=max_support)\n",
    "\n",
    "    test_root = BASE_DIR / cls / \"test\"\n",
    "    y_true, y_score = [], []\n",
    "    normal_prompts, anomaly_prompts = build_cpe_prompts(cls)\n",
    "\n",
    "    # 收集所有測試影像\n",
    "    all_imgs = []\n",
    "    for sub in [\"good\"] + [d.name for d in test_root.iterdir() if d.is_dir() and d.name != \"good\"]:\n",
    "        for img_path in (test_root / sub).glob(\"*.png\"):\n",
    "            all_imgs.append((img_path, sub))\n",
    "    \n",
    "    for img_path, sub in tqdm(all_imgs, desc=f\"[{cls}] 推理中\", unit=\"img\"):\n",
    "        label = 0 if sub == \"good\" else 1\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "\n",
    "        # 多尺度融合\n",
    "        score_maps = []\n",
    "        for ws in WINDOW_SIZES:\n",
    "            score_map = extract_winclip_plus_features(\n",
    "                model, preprocess, img, normal_prompts, anomaly_prompts,\n",
    "                emb_norm_support=emb_norm_support,\n",
    "                window_size=ws, stride=STRIDE\n",
    "            )\n",
    "            score_maps.append(score_map)\n",
    "\n",
    "        final_map = np.mean(score_maps, axis=0)\n",
    "\n",
    "        # Top-k pooling (前5% pixel 平均)\n",
    "        flat = final_map.flatten()\n",
    "        k = max(1, int(len(flat) * TOPK_RATIO))\n",
    "        topk = np.partition(flat, -k)[-k:]\n",
    "        score = topk.mean()\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_score.append(score)\n",
    "\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "# ================== Pipeline 主程式 ==================\n",
    "results = []\n",
    "for cls in MVTEC2_CLASSES:\n",
    "    print(f\"\\n>>> Class: {cls} (Backbone={BACKBONE})\")\n",
    "    auroc = run_inference_winclip_plus(cls, max_support=9999)  # 預設用全部 good 當 support\n",
    "    print(f\"[RESULT] {cls} AUROC={auroc:.4f}\")\n",
    "    results.append({\"class\": cls, \"auroc\": auroc, \"backbone\": BACKBONE})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"Results saved to {RESULTS_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c6adb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Class: two (Backbone=ViT-B-16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[two] 推理中: 100%|██████████| 230/230 [09:29<00:00,  2.47s/img]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] two AUROC=0.7510\n",
      "Results saved to C:\\Users\\anywhere4090\\Desktop\\DDAD-main\\mvtec2_winclip_plus_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# mvtec2_winclip_plus_pipeline.py\n",
    "# WinCLIP+ Pipeline：CPE prompt + Few-shot Normal Support + Top-k pooling + backbone selection\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import open_clip   # pip install open_clip_torch\n",
    "\n",
    "# ================== 全域設定 ==================\n",
    "MVTEC2_CLASSES = [\"two\"]\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\\dataset\\btad\")\n",
    "OUT_BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\DDAD-main\")\n",
    "RESULTS_CSV = OUT_BASE_DIR / \"mvtec2_winclip_plus_results.csv\"\n",
    "\n",
    "IMGSIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Backbone 可選: \"ViT-B-16\" / \"ViT-L-14\"\n",
    "BACKBONE = \"ViT-B-16\"\n",
    "PRETRAINED = \"laion400m_e32\" if BACKBONE == \"ViT-B-16\" else \"laion2b_s32b_b82k\"\n",
    "\n",
    "# Window/patch 設定\n",
    "WINDOW_SIZES = [32, 48, 64]\n",
    "STRIDE = 16\n",
    "\n",
    "# Top-k pooling 設定\n",
    "TOPK_RATIO = 0.05  # 取前 5%\n",
    "\n",
    "# ================== CPE Prompt ==================\n",
    "STATE_WORDS_NORMAL = [\"flawless\", \"intact\", \"perfect\", \"clean\", \"good\"]\n",
    "STATE_WORDS_ANOM = [\"broken\", \"cracked\", \"damaged\", \"scratched\", \"defective\", \"faulty\"]\n",
    "TEMPLATES = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of a {} for visual inspection\"\n",
    "]\n",
    "\n",
    "def build_cpe_prompts(cls_name):\n",
    "    normal_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_NORMAL for t in TEMPLATES]\n",
    "    anomaly_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_ANOM for t in TEMPLATES]\n",
    "    return normal_prompts, anomaly_prompts\n",
    "\n",
    "# ================== 建立 Support Embeddings ==================\n",
    "def build_support_embeddings(model, preprocess, cls, max_support=9999):\n",
    "    \"\"\"使用 train/good 生成 support embedding (可用全部正常影像)\"\"\"\n",
    "    support_dir = BASE_DIR / cls / \"train\" / \"good\"\n",
    "    support_imgs = sorted(list(support_dir.glob(\"*.png\")))[:max_support]\n",
    "    support_embs = []\n",
    "\n",
    "    for img_path in support_imgs:\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "        tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            emb = model.encode_image(tensor)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        support_embs.append(emb)\n",
    "\n",
    "    if len(support_embs) > 0:\n",
    "        return torch.cat(support_embs, dim=0)  # shape = [N, D]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# ================== WinCLIP+ 特徵抽取 ==================\n",
    "def extract_winclip_plus_features(model, preprocess, image, normal_prompts, anomaly_prompts,\n",
    "                                  emb_norm_support=None, window_size=32, stride=16):\n",
    "    W, H = image.size\n",
    "    scores = np.zeros((H, W))\n",
    "    counts = np.zeros((H, W))\n",
    "\n",
    "    # encode text prompts\n",
    "    with torch.no_grad():\n",
    "        txt_norm = model.encode_text(open_clip.tokenize(normal_prompts).to(DEVICE))\n",
    "        txt_anom = model.encode_text(open_clip.tokenize(anomaly_prompts).to(DEVICE))\n",
    "        txt_norm = txt_norm / txt_norm.norm(dim=-1, keepdim=True)\n",
    "        txt_anom = txt_anom / txt_anom.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # sliding window\n",
    "    for y in range(0, H - window_size + 1, stride):\n",
    "        for x in range(0, W - window_size + 1, stride):\n",
    "            crop = image.crop((x, y, x + window_size, y + window_size))\n",
    "            crop_tensor = preprocess(crop).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img_emb = model.encode_image(crop_tensor)\n",
    "                img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # normal 相似度 (文字 + 支援影像)\n",
    "            sim_norm_text = (img_emb @ txt_norm.T).max().item()\n",
    "            sim_norm_support = (img_emb @ emb_norm_support.T).max().item() if emb_norm_support is not None else -1e9\n",
    "            sim_norm = max(sim_norm_text, sim_norm_support)\n",
    "\n",
    "            # anomaly 相似度\n",
    "            sim_anom = (img_emb @ txt_anom.T).max().item()\n",
    "\n",
    "            anomaly_score = sim_anom - sim_norm  # 越大越異常\n",
    "\n",
    "            scores[y:y + window_size, x:x + window_size] += anomaly_score\n",
    "            counts[y:y + window_size, x:x + window_size] += 1\n",
    "\n",
    "    return scores / (counts + 1e-6)\n",
    "\n",
    "# ================== 推理 (WinCLIP+) ==================\n",
    "def run_inference_winclip_plus(cls, max_support=9999):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        BACKBONE, pretrained=PRETRAINED\n",
    "    )\n",
    "    model = model.to(DEVICE).eval()\n",
    "\n",
    "    # 建立 support embeddings\n",
    "    emb_norm_support = build_support_embeddings(model, preprocess, cls, max_support=max_support)\n",
    "\n",
    "    test_root = BASE_DIR / cls / \"test\"\n",
    "    y_true, y_score = [], []\n",
    "    normal_prompts, anomaly_prompts = build_cpe_prompts(cls)\n",
    "\n",
    "    # 收集所有測試影像\n",
    "    all_imgs = []\n",
    "    for sub in [\"good\"] + [d.name for d in test_root.iterdir() if d.is_dir() and d.name != \"good\"]:\n",
    "        for img_path in (test_root / sub).glob(\"*.png\"):\n",
    "            all_imgs.append((img_path, sub))\n",
    "    \n",
    "    for img_path, sub in tqdm(all_imgs, desc=f\"[{cls}] 推理中\", unit=\"img\"):\n",
    "        label = 0 if sub == \"good\" else 1\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "\n",
    "        # 多尺度融合\n",
    "        score_maps = []\n",
    "        for ws in WINDOW_SIZES:\n",
    "            score_map = extract_winclip_plus_features(\n",
    "                model, preprocess, img, normal_prompts, anomaly_prompts,\n",
    "                emb_norm_support=emb_norm_support,\n",
    "                window_size=ws, stride=STRIDE\n",
    "            )\n",
    "            score_maps.append(score_map)\n",
    "\n",
    "        final_map = np.mean(score_maps, axis=0)\n",
    "\n",
    "        # Top-k pooling (前5% pixel 平均)\n",
    "        flat = final_map.flatten()\n",
    "        k = max(1, int(len(flat) * TOPK_RATIO))\n",
    "        topk = np.partition(flat, -k)[-k:]\n",
    "        score = topk.mean()\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_score.append(score)\n",
    "\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "# ================== Pipeline 主程式 ==================\n",
    "results = []\n",
    "for cls in MVTEC2_CLASSES:\n",
    "    print(f\"\\n>>> Class: {cls} (Backbone={BACKBONE})\")\n",
    "    auroc = run_inference_winclip_plus(cls, max_support=9999)  # 預設用全部 good 當 support\n",
    "    print(f\"[RESULT] {cls} AUROC={auroc:.4f}\")\n",
    "    results.append({\"class\": cls, \"auroc\": auroc, \"backbone\": BACKBONE})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"Results saved to {RESULTS_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4599846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Class: three (Backbone=ViT-B-16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[three] 推理中: 100%|██████████| 441/441 [17:48<00:00,  2.42s/img]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] three AUROC=0.6763\n",
      "Results saved to C:\\Users\\anywhere4090\\Desktop\\DDAD-main\\mvtec2_winclip_plus_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# mvtec2_winclip_plus_pipeline.py\n",
    "# WinCLIP+ Pipeline：CPE prompt + Few-shot Normal Support + Top-k pooling + backbone selection\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import open_clip   # pip install open_clip_torch\n",
    "\n",
    "# ================== 全域設定 ==================\n",
    "MVTEC2_CLASSES = [\"three\"]\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\\dataset\\btad\")\n",
    "OUT_BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\DDAD-main\")\n",
    "RESULTS_CSV = OUT_BASE_DIR / \"mvtec2_winclip_plus_results.csv\"\n",
    "\n",
    "IMGSIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Backbone 可選: \"ViT-B-16\" / \"ViT-L-14\"\n",
    "BACKBONE = \"ViT-L-14\"\n",
    "PRETRAINED = \"laion400m_e32\" if BACKBONE == \"ViT-B-16\" else \"laion2b_s32b_b82k\"\n",
    "\n",
    "# Window/patch 設定\n",
    "WINDOW_SIZES = [32, 48, 64]\n",
    "STRIDE = 16\n",
    "\n",
    "# Top-k pooling 設定\n",
    "TOPK_RATIO = 0.05  # 取前 5%\n",
    "\n",
    "# ================== CPE Prompt ==================\n",
    "STATE_WORDS_NORMAL = [\"flawless\", \"intact\", \"perfect\", \"clean\", \"good\"]\n",
    "STATE_WORDS_ANOM = [\"broken\", \"cracked\", \"damaged\", \"scratched\", \"defective\", \"faulty\"]\n",
    "TEMPLATES = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of a {} for visual inspection\"\n",
    "]\n",
    "\n",
    "def build_cpe_prompts(cls_name):\n",
    "    normal_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_NORMAL for t in TEMPLATES]\n",
    "    anomaly_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_ANOM for t in TEMPLATES]\n",
    "    return normal_prompts, anomaly_prompts\n",
    "\n",
    "# ================== 建立 Support Embeddings ==================\n",
    "def build_support_embeddings(model, preprocess, cls, max_support=9999):\n",
    "    \"\"\"使用 train/good 生成 support embedding (可用全部正常影像)\"\"\"\n",
    "    support_dir = BASE_DIR / cls / \"train\" / \"good\"\n",
    "    support_imgs = sorted(list(support_dir.glob(\"*.png\")))[:max_support]\n",
    "    support_embs = []\n",
    "\n",
    "    for img_path in support_imgs:\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "        tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            emb = model.encode_image(tensor)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        support_embs.append(emb)\n",
    "\n",
    "    if len(support_embs) > 0:\n",
    "        return torch.cat(support_embs, dim=0)  # shape = [N, D]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# ================== WinCLIP+ 特徵抽取 ==================\n",
    "def extract_winclip_plus_features(model, preprocess, image, normal_prompts, anomaly_prompts,\n",
    "                                  emb_norm_support=None, window_size=32, stride=16):\n",
    "    W, H = image.size\n",
    "    scores = np.zeros((H, W))\n",
    "    counts = np.zeros((H, W))\n",
    "\n",
    "    # encode text prompts\n",
    "    with torch.no_grad():\n",
    "        txt_norm = model.encode_text(open_clip.tokenize(normal_prompts).to(DEVICE))\n",
    "        txt_anom = model.encode_text(open_clip.tokenize(anomaly_prompts).to(DEVICE))\n",
    "        txt_norm = txt_norm / txt_norm.norm(dim=-1, keepdim=True)\n",
    "        txt_anom = txt_anom / txt_anom.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # sliding window\n",
    "    for y in range(0, H - window_size + 1, stride):\n",
    "        for x in range(0, W - window_size + 1, stride):\n",
    "            crop = image.crop((x, y, x + window_size, y + window_size))\n",
    "            crop_tensor = preprocess(crop).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img_emb = model.encode_image(crop_tensor)\n",
    "                img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # normal 相似度 (文字 + 支援影像)\n",
    "            sim_norm_text = (img_emb @ txt_norm.T).max().item()\n",
    "            sim_norm_support = (img_emb @ emb_norm_support.T).max().item() if emb_norm_support is not None else -1e9\n",
    "            sim_norm = max(sim_norm_text, sim_norm_support)\n",
    "\n",
    "            # anomaly 相似度\n",
    "            sim_anom = (img_emb @ txt_anom.T).max().item()\n",
    "\n",
    "            anomaly_score = sim_anom - sim_norm  # 越大越異常\n",
    "\n",
    "            scores[y:y + window_size, x:x + window_size] += anomaly_score\n",
    "            counts[y:y + window_size, x:x + window_size] += 1\n",
    "\n",
    "    return scores / (counts + 1e-6)\n",
    "\n",
    "# ================== 推理 (WinCLIP+) ==================\n",
    "def run_inference_winclip_plus(cls, max_support=9999):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        BACKBONE, pretrained=PRETRAINED\n",
    "    )\n",
    "    model = model.to(DEVICE).eval()\n",
    "\n",
    "    # 建立 support embeddings\n",
    "    emb_norm_support = build_support_embeddings(model, preprocess, cls, max_support=max_support)\n",
    "\n",
    "    test_root = BASE_DIR / cls / \"test\"\n",
    "    y_true, y_score = [], []\n",
    "    normal_prompts, anomaly_prompts = build_cpe_prompts(cls)\n",
    "\n",
    "    # 收集所有測試影像\n",
    "    all_imgs = []\n",
    "    for sub in [\"good\"] + [d.name for d in test_root.iterdir() if d.is_dir() and d.name != \"good\"]:\n",
    "        for img_path in (test_root / sub).glob(\"*.png\"):\n",
    "            all_imgs.append((img_path, sub))\n",
    "    \n",
    "    for img_path, sub in tqdm(all_imgs, desc=f\"[{cls}] 推理中\", unit=\"img\"):\n",
    "        label = 0 if sub == \"good\" else 1\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "\n",
    "        # 多尺度融合\n",
    "        score_maps = []\n",
    "        for ws in WINDOW_SIZES:\n",
    "            score_map = extract_winclip_plus_features(\n",
    "                model, preprocess, img, normal_prompts, anomaly_prompts,\n",
    "                emb_norm_support=emb_norm_support,\n",
    "                window_size=ws, stride=STRIDE\n",
    "            )\n",
    "            score_maps.append(score_map)\n",
    "\n",
    "        final_map = np.mean(score_maps, axis=0)\n",
    "\n",
    "        # Top-k pooling (前5% pixel 平均)\n",
    "        flat = final_map.flatten()\n",
    "        k = max(1, int(len(flat) * TOPK_RATIO))\n",
    "        topk = np.partition(flat, -k)[-k:]\n",
    "        score = topk.mean()\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_score.append(score)\n",
    "\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "# ================== Pipeline 主程式 ==================\n",
    "results = []\n",
    "for cls in MVTEC2_CLASSES:\n",
    "    print(f\"\\n>>> Class: {cls} (Backbone={BACKBONE})\")\n",
    "    auroc = run_inference_winclip_plus(cls, max_support=9999)  # 預設用全部 good 當 support\n",
    "    print(f\"[RESULT] {cls} AUROC={auroc:.4f}\")\n",
    "    results.append({\"class\": cls, \"auroc\": auroc, \"backbone\": BACKBONE})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"Results saved to {RESULTS_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed8614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\begin{table}[H]\n",
    "\\centering\n",
    "\\caption{MVTec AD 2 dataset: Image AUROC (\\%) across different CLIP training strategies.}\n",
    "\\label{tab:mvtec2_clip_perclass}\n",
    "\\resizebox{\\linewidth}{!}{\n",
    "\\begin{tabular}{l c c c c c c c c c}\n",
    "\\toprule\n",
    "Setting & can & fabric & fruit\\_jelly & rice & sheet\\_metal & vial & wallplugs & walnuts & Average \\\\\n",
    "\\midrule\n",
    "Only CLIP (only good) & 44.6451 & 38.05 & 49.25 & 51.16 & 42.12 & 66.1497 & 43.03 & 58.51 & 49.1011 \\\\\n",
    "Only CLIP (good + 1-fold augmentation + 1-fold duplication) & 39.7531 & 33.97 & 55.83 & 52.54 & 32.64 & 66.18 & 52.39 & 49.74 & 47.3804 \\\\\n",
    "Only CLIP (good + 2-fold augmentation + 2-fold duplication) & 45.52 & 37.21 & 49.83 & 51.24 & 29.26 & 55.16 & 58.09 & 56.4074 & 47.8397 \\\\\n",
    "Only CLIP (good + 3-fold augmentation + 3-fold duplication) & 46.5741 & 43.569 & 51.833 & 47.9894 & 37.6852 & 67.1293 & 59.7778 & 63.2778 & \\textbf{52.9795} \\\\\n",
    "Only CLIP (good + 4-fold augmentation + 4-fold duplication) & 47.3765 & 38.67 & 54.9167 & 51.8254 & 33.4259 & 60.1361 & 64.4074 & 48.9259 & 49.7105 \\\\\n",
    "Only CLIP (good + 5-fold augmentation + 5-fold duplication) & 49.25 & 37.3064 & 54.5833 & 45.7011 & 35.3241 & 60.6259 & 65.9259 & 48.6481 & 49.6706 \\\\\n",
    "\\textbf{Diffusion baseline + normalization anomaly score and guidance by scale + CLIP (good + 3-fold augmentation + 3-fold duplication)} & 73.61 & 80.37 & 95.33 & 75.93 & 60.83 & 67.14 & 58.33 & 98.56 & \\textbf{76.26} \\\\\n",
    "\\textbf{Base (diffusion, only image)} & 73.04 & 53.45 & 96.58 & 75.13 & 72.96 & 79.81 & 66.20 & 79.44 & \\textbf{74.58} \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "}\n",
    "\\end{table}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897853f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Class: bagel (Backbone=ViT-L-14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d20ff5440874a91ae7ea238599f9b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anywhere4090\\.conda\\envs\\cpsu\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anywhere4090\\.cache\\huggingface\\hub\\models--laion--CLIP-ViT-L-14-laion2B-s32B-b82K. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "[bagel] 推理中: 100%|██████████| 110/110 [11:02<00:00,  6.02s/img]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] bagel Image-AUROC=0.8079, Pixel-AUROC=0.4367, PRO=0.4812\n",
      "Results saved to C:\\Users\\anywhere4090\\Desktop\\DDAD-main\\mvtec2_winclip_plus_seg_results.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# mvtec2_winclip_plus_seg_pipeline.py\n",
    "# WinCLIP+ Segmentation 版：\n",
    "# - Classification: Image AUROC (Top-k pooling)\n",
    "# - Segmentation: Pixel AUROC, PRO\n",
    "# - Few-shot Normal Support 可選\n",
    "# - 多尺度 sliding-window feature extraction\n",
    "# - 含 tqdm 進度條\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import open_clip   # pip install open_clip_torch\n",
    "from skimage import measure\n",
    "\n",
    "# ================== 全域設定 ==================\n",
    "MVTEC2_CLASSES = [\"bagel\"]\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\\dataset\\newmvtec3d\")\n",
    "OUT_BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\DDAD-main\")\n",
    "RESULTS_CSV = OUT_BASE_DIR / \"mvtec2_winclip_plus_seg_results.csv\"\n",
    "\n",
    "IMGSIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Backbone 可選: \"ViT-B-16\" / \"ViT-L-14\"\n",
    "BACKBONE = \"ViT-L-14\"\n",
    "PRETRAINED = \"laion400m_e32\" if BACKBONE == \"ViT-B-16\" else \"laion2b_s32b_b82k\"\n",
    "\n",
    "# Window/patch 設定\n",
    "WINDOW_SIZES = [32, 48, 64]\n",
    "STRIDE = 16\n",
    "\n",
    "# Top-k pooling 設定\n",
    "TOPK_RATIO = 0.05  # 取前 5%\n",
    "\n",
    "# Few-shot Support\n",
    "SUPPORT_MAX = 5  # 可調：取多少張 train/good 作 support\n",
    "\n",
    "# ================== CPE Prompt ==================\n",
    "STATE_WORDS_NORMAL = [\"flawless\", \"intact\", \"perfect\", \"clean\", \"good\"]\n",
    "STATE_WORDS_ANOM = [\"broken\", \"cracked\", \"damaged\", \"scratched\", \"defective\", \"faulty\"]\n",
    "TEMPLATES = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of a {} for visual inspection\"\n",
    "]\n",
    "\n",
    "def build_cpe_prompts(cls_name):\n",
    "    normal_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_NORMAL for t in TEMPLATES]\n",
    "    anomaly_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_ANOM for t in TEMPLATES]\n",
    "    return normal_prompts, anomaly_prompts\n",
    "\n",
    "# ================== 建立 Support Embeddings ==================\n",
    "def build_support_embeddings(model, preprocess, cls, max_support=SUPPORT_MAX):\n",
    "    \"\"\"從 train/good 抽少量正常影像，生成 support embedding\"\"\"\n",
    "    support_dir = BASE_DIR / cls / \"train\" / \"good\"\n",
    "    support_imgs = sorted(list(support_dir.glob(\"*.png\")))[:max_support]\n",
    "    support_embs = []\n",
    "\n",
    "    for img_path in support_imgs:\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "        tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            emb = model.encode_image(tensor)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        support_embs.append(emb)\n",
    "\n",
    "    if len(support_embs) > 0:\n",
    "        return torch.cat(support_embs, dim=0)  # shape = [N, D]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# ================== WinCLIP+ 特徵抽取 ==================\n",
    "def extract_winclip_plus_features(model, preprocess, image, normal_prompts, anomaly_prompts,\n",
    "                                  emb_norm_support=None, window_size=32, stride=16):\n",
    "    W, H = image.size\n",
    "    scores = np.zeros((H, W))\n",
    "    counts = np.zeros((H, W))\n",
    "\n",
    "    # encode text prompts\n",
    "    with torch.no_grad():\n",
    "        txt_norm = model.encode_text(open_clip.tokenize(normal_prompts).to(DEVICE))\n",
    "        txt_anom = model.encode_text(open_clip.tokenize(anomaly_prompts).to(DEVICE))\n",
    "        txt_norm = txt_norm / txt_norm.norm(dim=-1, keepdim=True)\n",
    "        txt_anom = txt_anom / txt_anom.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # sliding window\n",
    "    for y in range(0, H - window_size + 1, stride):\n",
    "        for x in range(0, W - window_size + 1, stride):\n",
    "            crop = image.crop((x, y, x + window_size, y + window_size))\n",
    "            crop_tensor = preprocess(crop).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img_emb = model.encode_image(crop_tensor)\n",
    "                img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # normal 相似度 (文字 + 支援影像)\n",
    "            sim_norm_text = (img_emb @ txt_norm.T).max().item()\n",
    "            sim_norm_support = (img_emb @ emb_norm_support.T).max().item() if emb_norm_support is not None else -1e9\n",
    "            sim_norm = max(sim_norm_text, sim_norm_support)\n",
    "\n",
    "            # anomaly 相似度\n",
    "            sim_anom = (img_emb @ txt_anom.T).max().item()\n",
    "\n",
    "            anomaly_score = sim_anom - sim_norm  # 越大越異常\n",
    "\n",
    "            scores[y:y + window_size, x:x + window_size] += anomaly_score\n",
    "            counts[y:y + window_size, x:x + window_size] += 1\n",
    "\n",
    "    return scores / (counts + 1e-6)\n",
    "\n",
    "# ================== 計算 PRO (Per-Region-Overlap) ==================\n",
    "def compute_pro(masks, heatmaps, num_th=50):\n",
    "    pros = []\n",
    "    for th in np.linspace(0, 1, num_th):\n",
    "        bin_preds = (heatmaps >= th).astype(np.uint8)\n",
    "        for mask, pred in zip(masks, bin_preds):\n",
    "            label_mask = measure.label(mask, connectivity=2)\n",
    "            regions = np.unique(label_mask)[1:]  # skip background\n",
    "            for r in regions:\n",
    "                region = (label_mask == r)\n",
    "                inter = (pred * region).sum()\n",
    "                union = region.sum()\n",
    "                if union > 0:\n",
    "                    pros.append(inter / union)\n",
    "    return np.mean(pros) if len(pros) > 0 else 0.0\n",
    "\n",
    "# ================== 推理 (WinCLIP+ Segmentation) ==================\n",
    "def run_inference_winclip_plus_seg(cls, max_support=SUPPORT_MAX):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        BACKBONE, pretrained=PRETRAINED\n",
    "    )\n",
    "    model = model.to(DEVICE).eval()\n",
    "\n",
    "    # 建立 support embeddings\n",
    "    emb_norm_support = build_support_embeddings(model, preprocess, cls, max_support=max_support)\n",
    "\n",
    "    test_root = BASE_DIR / cls / \"test\"\n",
    "    gt_root = BASE_DIR / cls / \"ground_truth\"\n",
    "\n",
    "    y_true, y_score = [], []\n",
    "    masks_all, maps_all = [], []\n",
    "\n",
    "    normal_prompts, anomaly_prompts = build_cpe_prompts(cls)\n",
    "\n",
    "    # 收集所有測試影像\n",
    "    all_imgs = []\n",
    "    for sub in [\"good\"] + [d.name for d in test_root.iterdir() if d.is_dir() and d.name != \"good\"]:\n",
    "        for img_path in (test_root / sub).glob(\"*.png\"):\n",
    "            all_imgs.append((img_path, sub))\n",
    "    \n",
    "    for img_path, sub in tqdm(all_imgs, desc=f\"[{cls}] 推理中\", unit=\"img\"):\n",
    "        label = 0 if sub == \"good\" else 1\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "\n",
    "        # 多尺度融合\n",
    "        score_maps = []\n",
    "        for ws in WINDOW_SIZES:\n",
    "            score_map = extract_winclip_plus_features(\n",
    "                model, preprocess, img, normal_prompts, anomaly_prompts,\n",
    "                emb_norm_support=emb_norm_support,\n",
    "                window_size=ws, stride=STRIDE\n",
    "            )\n",
    "            score_maps.append(score_map)\n",
    "\n",
    "        final_map = np.mean(score_maps, axis=0)\n",
    "\n",
    "        # === Classification score (Top-k pooling) ===\n",
    "        flat = final_map.flatten()\n",
    "        k = max(1, int(len(flat) * TOPK_RATIO))\n",
    "        topk = np.partition(flat, -k)[-k:]\n",
    "        score = topk.mean()\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_score.append(score)\n",
    "\n",
    "        # === Segmentation (only anomaly has mask) ===\n",
    "        if label == 1:\n",
    "            mask_path = gt_root / sub / img_path.name\n",
    "            if mask_path.exists():\n",
    "                mask = Image.open(mask_path).convert(\"L\").resize(final_map.shape[::-1])\n",
    "                mask = (np.array(mask) > 127).astype(np.uint8)\n",
    "                masks_all.append(mask)\n",
    "                maps_all.append((final_map - final_map.min()) / (final_map.max() - final_map.min() + 1e-6))\n",
    "\n",
    "    # Image-level AUROC\n",
    "    img_auroc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    # Pixel-level AUROC & PRO\n",
    "    if len(masks_all) > 0:\n",
    "        masks_all = np.array(masks_all)\n",
    "        maps_all = np.array(maps_all)\n",
    "        px_auroc = roc_auc_score(masks_all.flatten(), maps_all.flatten())\n",
    "        pro = compute_pro(masks_all, maps_all)\n",
    "    else:\n",
    "        px_auroc, pro = np.nan, np.nan\n",
    "\n",
    "    return img_auroc, px_auroc, pro\n",
    "\n",
    "# ================== Pipeline 主程式 ==================\n",
    "results = []\n",
    "for cls in MVTEC2_CLASSES:\n",
    "    print(f\"\\n>>> Class: {cls} (Backbone={BACKBONE})\")\n",
    "    img_auroc, px_auroc, pro = run_inference_winclip_plus_seg(cls, max_support=SUPPORT_MAX)\n",
    "    print(f\"[RESULT] {cls} Image-AUROC={img_auroc:.4f}, Pixel-AUROC={px_auroc:.4f}, PRO={pro:.4f}\")\n",
    "    results.append({\n",
    "        \"class\": cls,\n",
    "        \"backbone\": BACKBONE,\n",
    "        \"support\": SUPPORT_MAX,\n",
    "        \"image_auroc\": img_auroc,\n",
    "        \"pixel_auroc\": px_auroc,\n",
    "        \"pro\": pro\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"Results saved to {RESULTS_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
