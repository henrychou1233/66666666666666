{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7031752a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Class: bagel (Backbone=ViT-L-14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[bagel] 推理中: 100%|██████████| 110/110 [10:58<00:00,  5.98s/img]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] bagel Image-AUROC=0.8079, Pixel-AUROC=0.4367, PRO=0.4812\n",
      "Results saved to C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\\mvtec2_winclip_plus_seg_results.csv\n",
      "WinCLIP scores saved to C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\\winclip_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# clipad2.py\n",
    "# WinCLIP+ Segmentation 版：\n",
    "# - Classification: Image AUROC (Top-k pooling)\n",
    "# - Segmentation: Pixel AUROC, PRO\n",
    "# - Few-shot Normal Support 可選\n",
    "# - 多尺度 sliding-window feature extraction\n",
    "# - 含 tqdm 進度條\n",
    "# - 輸出 CSV: class, image, score\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import open_clip   # pip install open_clip_torch\n",
    "from skimage import measure\n",
    "\n",
    "# ================== 全域設定 ==================\n",
    "MVTEC2_CLASSES = [\"bagel\"]\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\\dataset\\newmvtec3d\")\n",
    "OUT_BASE_DIR = Path(r\"C:\\Users\\anywhere4090\\Desktop\\0902 finalcode\")\n",
    "RESULTS_CSV = OUT_BASE_DIR / \"mvtec2_winclip_plus_seg_results.csv\"\n",
    "SCORES_CSV = OUT_BASE_DIR / \"winclip_scores.csv\"\n",
    "\n",
    "IMGSIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Backbone 可選: \"ViT-B-16\" / \"ViT-L-14\"\n",
    "BACKBONE = \"ViT-L-14\"\n",
    "PRETRAINED = \"laion400m_e32\" if BACKBONE == \"ViT-B-16\" else \"laion2b_s32b_b82k\"\n",
    "\n",
    "# Window/patch 設定\n",
    "WINDOW_SIZES = [32, 48, 64]\n",
    "STRIDE = 16\n",
    "\n",
    "# Top-k pooling 設定\n",
    "TOPK_RATIO = 0.05  # 取前 5%\n",
    "\n",
    "# Few-shot Support\n",
    "SUPPORT_MAX = 5  # 可調：取多少張 train/good 作 support\n",
    "\n",
    "# ================== CPE Prompt ==================\n",
    "STATE_WORDS_NORMAL = [\"flawless\", \"intact\", \"perfect\", \"clean\", \"good\"]\n",
    "STATE_WORDS_ANOM = [\"broken\", \"cracked\", \"damaged\", \"scratched\", \"defective\", \"faulty\"]\n",
    "TEMPLATES = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of a {} for visual inspection\"\n",
    "]\n",
    "\n",
    "def build_cpe_prompts(cls_name):\n",
    "    normal_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_NORMAL for t in TEMPLATES]\n",
    "    anomaly_prompts = [t.format(w + \" \" + cls_name) for w in STATE_WORDS_ANOM for t in TEMPLATES]\n",
    "    return normal_prompts, anomaly_prompts\n",
    "\n",
    "# ================== 建立 Support Embeddings ==================\n",
    "def build_support_embeddings(model, preprocess, cls, max_support=SUPPORT_MAX):\n",
    "    support_dir = BASE_DIR / cls / \"train\" / \"good\"\n",
    "    support_imgs = sorted(list(support_dir.glob(\"*.png\")))[:max_support]\n",
    "    support_embs = []\n",
    "    for img_path in support_imgs:\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "        tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            emb = model.encode_image(tensor)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        support_embs.append(emb)\n",
    "    return torch.cat(support_embs, dim=0) if len(support_embs) > 0 else None\n",
    "\n",
    "# ================== WinCLIP+ 特徵抽取 ==================\n",
    "def extract_winclip_plus_features(model, preprocess, image, normal_prompts, anomaly_prompts,\n",
    "                                  emb_norm_support=None, window_size=32, stride=16):\n",
    "    W, H = image.size\n",
    "    scores = np.zeros((H, W))\n",
    "    counts = np.zeros((H, W))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        txt_norm = model.encode_text(open_clip.tokenize(normal_prompts).to(DEVICE))\n",
    "        txt_anom = model.encode_text(open_clip.tokenize(anomaly_prompts).to(DEVICE))\n",
    "        txt_norm = txt_norm / txt_norm.norm(dim=-1, keepdim=True)\n",
    "        txt_anom = txt_anom / txt_anom.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    for y in range(0, H - window_size + 1, stride):\n",
    "        for x in range(0, W - window_size + 1, stride):\n",
    "            crop = image.crop((x, y, x + window_size, y + window_size))\n",
    "            crop_tensor = preprocess(crop).unsqueeze(0).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                img_emb = model.encode_image(crop_tensor)\n",
    "                img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            sim_norm_text = (img_emb @ txt_norm.T).max().item()\n",
    "            sim_norm_support = (img_emb @ emb_norm_support.T).max().item() if emb_norm_support is not None else -1e9\n",
    "            sim_norm = max(sim_norm_text, sim_norm_support)\n",
    "\n",
    "            sim_anom = (img_emb @ txt_anom.T).max().item()\n",
    "            anomaly_score = sim_anom - sim_norm\n",
    "\n",
    "            scores[y:y + window_size, x:x + window_size] += anomaly_score\n",
    "            counts[y:y + window_size, x:x + window_size] += 1\n",
    "\n",
    "    return scores / (counts + 1e-6)\n",
    "\n",
    "# ================== 計算 PRO ==================\n",
    "def compute_pro(masks, heatmaps, num_th=50):\n",
    "    pros = []\n",
    "    for th in np.linspace(0, 1, num_th):\n",
    "        bin_preds = (heatmaps >= th).astype(np.uint8)\n",
    "        for mask, pred in zip(masks, bin_preds):\n",
    "            label_mask = measure.label(mask, connectivity=2)\n",
    "            regions = np.unique(label_mask)[1:]\n",
    "            for r in regions:\n",
    "                region = (label_mask == r)\n",
    "                inter = (pred * region).sum()\n",
    "                union = region.sum()\n",
    "                if union > 0:\n",
    "                    pros.append(inter / union)\n",
    "    return np.mean(pros) if len(pros) > 0 else 0.0\n",
    "\n",
    "# ================== 推理 ==================\n",
    "def run_inference_winclip_plus_seg(cls, max_support=SUPPORT_MAX):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "        BACKBONE, pretrained=PRETRAINED\n",
    "    )\n",
    "    model = model.to(DEVICE).eval()\n",
    "\n",
    "    emb_norm_support = build_support_embeddings(model, preprocess, cls, max_support=max_support)\n",
    "\n",
    "    test_root = BASE_DIR / cls / \"test\"\n",
    "    gt_root = BASE_DIR / cls / \"ground_truth\"\n",
    "\n",
    "    y_true, y_score = [], []\n",
    "    masks_all, maps_all = [], []\n",
    "    normal_prompts, anomaly_prompts = build_cpe_prompts(cls)\n",
    "\n",
    "    all_imgs = []\n",
    "    for sub in [\"good\"] + [d.name for d in test_root.iterdir() if d.is_dir() and d.name != \"good\"]:\n",
    "        for img_path in (test_root / sub).glob(\"*.png\"):\n",
    "            all_imgs.append((img_path, sub))\n",
    "\n",
    "    scores_dict = []\n",
    "\n",
    "    for img_path, sub in tqdm(all_imgs, desc=f\"[{cls}] 推理中\", unit=\"img\"):\n",
    "        label = 0 if sub == \"good\" else 1\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((IMGSIZE, IMGSIZE))\n",
    "\n",
    "        score_maps = []\n",
    "        for ws in WINDOW_SIZES:\n",
    "            score_map = extract_winclip_plus_features(\n",
    "                model, preprocess, img, normal_prompts, anomaly_prompts,\n",
    "                emb_norm_support=emb_norm_support,\n",
    "                window_size=ws, stride=STRIDE\n",
    "            )\n",
    "            score_maps.append(score_map)\n",
    "\n",
    "        final_map = np.mean(score_maps, axis=0)\n",
    "        flat = final_map.flatten()\n",
    "        k = max(1, int(len(flat) * TOPK_RATIO))\n",
    "        topk = np.partition(flat, -k)[-k:]\n",
    "        score = topk.mean()\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_score.append(score)\n",
    "        scores_dict.append({\"class\": cls, \"image\": img_path.name, \"score\": score})\n",
    "\n",
    "        if label == 1:\n",
    "            mask_path = gt_root / sub / img_path.name\n",
    "            if mask_path.exists():\n",
    "                mask = Image.open(mask_path).convert(\"L\").resize(final_map.shape[::-1])\n",
    "                mask = (np.array(mask) > 127).astype(np.uint8)\n",
    "                masks_all.append(mask)\n",
    "                maps_all.append((final_map - final_map.min()) / (final_map.max() - final_map.min() + 1e-6))\n",
    "\n",
    "    img_auroc = roc_auc_score(y_true, y_score)\n",
    "    if len(masks_all) > 0:\n",
    "        masks_all = np.array(masks_all)\n",
    "        maps_all = np.array(maps_all)\n",
    "        px_auroc = roc_auc_score(masks_all.flatten(), maps_all.flatten())\n",
    "        pro = compute_pro(masks_all, maps_all)\n",
    "    else:\n",
    "        px_auroc, pro = np.nan, np.nan\n",
    "\n",
    "    return img_auroc, px_auroc, pro, scores_dict\n",
    "\n",
    "# ================== Pipeline 主程式 ==================\n",
    "results, all_scores = [], []\n",
    "for cls in MVTEC2_CLASSES:\n",
    "    print(f\"\\n>>> Class: {cls} (Backbone={BACKBONE})\")\n",
    "    img_auroc, px_auroc, pro, scores_dict = run_inference_winclip_plus_seg(cls, max_support=SUPPORT_MAX)\n",
    "    print(f\"[RESULT] {cls} Image-AUROC={img_auroc:.4f}, Pixel-AUROC={px_auroc:.4f}, PRO={pro:.4f}\")\n",
    "    results.append({\n",
    "        \"class\": cls,\n",
    "        \"backbone\": BACKBONE,\n",
    "        \"support\": SUPPORT_MAX,\n",
    "        \"image_auroc\": img_auroc,\n",
    "        \"pixel_auroc\": px_auroc,\n",
    "        \"pro\": pro\n",
    "    })\n",
    "    all_scores.extend(scores_dict)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RESULTS_CSV, index=False)\n",
    "print(f\"Results saved to {RESULTS_CSV}\")\n",
    "\n",
    "df_scores = pd.DataFrame(all_scores)\n",
    "df_scores.to_csv(SCORES_CSV, index=False)\n",
    "print(f\"WinCLIP scores saved to {SCORES_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
